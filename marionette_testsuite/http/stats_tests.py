#!/usr/bin/env python
# coding: utf-8

import argparse
import os
import fnmatch
import collections

import numpy as np
import scipy.stats as stats
import scipy.spatial as spatial

import pyshark


#Walk through given path and find all pcap files
def expand_files(path):

    files = []

    for root, dirnames, filenames in os.walk(path):
        for filename in fnmatch.filter(filenames, '*.pcap'):
            full_path = os.path.join(root,filename)
            files.append(full_path)

    return files

#Go through given list of files and find sequence of HTTP response lengths
# This is focused on HTTP for the moment, but may be updated with different 
# display filters and packet attribute.
def get_msg_lens(files):

    stat_counts = []
    for filename in files:
        cap = pyshark.FileCapture(filename, display_filter='http.response')
        for pkt in cap:
            if hasattr(pkt['http'],'content_length'):
                resp_len = int(pkt['http'].content_length)
                if resp_len > 0:
                    stat_counts.append(resp_len)

    return stat_counts

#Go through given list of files and calculate the number of HTTP requests in 
# each TCP connection. TCP connection defined by 4-tuple.  Output stats are 
# based on the distribution of number of messages per connection.
def get_msg_per_conn(files):

    conn_count = []
    for filename in files:
        cap = pyshark.FileCapture(filename, display_filter='http.request')
        for pkt in cap:
            if 'ip' in pkt:
                hash_key = hash((pkt['ip'].src, pkt['ip'].dst, 
                    pkt['tcp'].srcport, pkt['tcp'].dstport))
                conn_count.append(hash_key)
    conn_count = collections.Counter(conn_count)

    stat_counts = []
    for hash_key in conn_count:
        stat_counts.append(conn_count[hash_key])

    return stat_counts

#Go through given list of files and assume each file is single "session."
# Count number of TCP connections in each "session" by looking for very 
# first HTTP request in each TCP connection.
def get_conn_per_session(files):

    stat_counts = []
    for filename in files:
        counter = 0.0
        cap = pyshark.FileCapture(filename, 
            display_filter='http.request and tcp.seq==1')
        for pkt in cap:
            counter += 1.0
        stat_counts.append(counter)

    return stat_counts

#Bin the sequence of values and normalize those counts.
def normalize(p, q):
    values = list(set(p).union(set(q)))
    values.sort()
    values.append(values[-1]+1)

    p_hist, bins = np.histogram(p, bins=values)
    p_hist = (p_hist*1.0)/sum(p_hist)

    q_hist, bins = np.histogram(q, bins=values)
    q_hist = (q_hist*1.0)/sum(q_hist)

    return p_hist, q_hist

if __name__ == '__main__':

    #Structure holds pointers to each feature extraction function.
    # Used to make test selection more flexible.
    tests = {'msg_per_conn':get_msg_per_conn, 'msg_len':get_msg_lens, 
            'conn_per_sess':get_conn_per_session}

    parser = argparse.ArgumentParser(description='Testsuite for statistical'
        'properties of network traffic generated by Marionette.')
    parser.add_argument('--train', '-t', dest='train', required=True,
        help='path to training files used as reference statistics')
    parser.add_argument('--test', '-T', dest='test',
        required=True, help='path to testing files produced by Marionette')
    parser.add_argument('--msg-len', dest='msg_len',
        action='store_true', required=False, 
        help='test distribution of message lengths')
    parser.add_argument('--msg-per-conn', dest='msg_per_conn',
        action='store_true', required=False, 
        help='test distribution of messages per connection')
    parser.add_argument('--conn-per-sess', dest='conn_per_sess',
        action='store_true', required=False, 
        help='test distribution of connections per session')
    args = parser.parse_args()

    #Gather the train and test files from the given paths
    train_files = expand_files(args.train)
    test_files = expand_files(args.test)

    for testname in tests:
        #If the user has selected this test (see structure above)...
        if vars(args)[testname] == True:
            print testname
            #Run the associated feature extraction function
            train_counts = tests[testname](train_files)
            test_counts = tests[testname](test_files)

            train_stats, test_stats = normalize(train_counts, test_counts)

            #Compute distances and output
            kl_div = stats.entropy(train_stats, test_stats, base=2)
            print "KL Divergence: ", kl_div

            l1_dist = spatial.distance.pdist([train_stats,test_stats],
                metric='cityblock')
            print "L1 Distance: ", l1_dist[0]
            
            ks_dist = stats.ks_2samp(train_stats, test_stats)
            print "Kolmogrov-Smirnov:", ks_dist
            print "-----------------"